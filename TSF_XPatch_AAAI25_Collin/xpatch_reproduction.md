# xPatch æ¨¡å‹å¤ç°ä¸ä»£ç è§£æ / xPatch Model Reproduction and Code Analysis

<div align="center">
  <h2><b>ğŸ”¬ (AAAI25) xPatch å¤ç°å®ç°ä¸æ¶æ„è§£æ / Reproduction Implementation and Architecture Analysis</b></h2>
  <h3><b>ğŸ“ å¤ç°æ•´ç†ï¼šCollin / Reproduced and Organized by: Collin</b></h3>
</div>

<div align="center">

[![](https://img.shields.io/badge/arXiv:2412.17323-B31B1B?logo=arxiv)](https://arxiv.org/pdf/2412.17323)
[![](https://img.shields.io/badge/AAAI'25-xPatch-deepgreen)](https://ojs.aaai.org/index.php/AAAI/article/view/34270)
[![](https://img.shields.io/badge/å¤ç°è€…-Collin-blue)]()
[![](https://img.shields.io/badge/çŠ¶æ€-å®Œæˆ-success)]()

</div>

## ğŸ“ é¡¹ç›®ç®€ä»‹ / Project Overview

**ä¸­æ–‡ç®€ä»‹ï¼š**
æœ¬æ–‡æ¡£è¯¦ç»†è®°å½•äº†å¯¹ AAAI 2025 è®ºæ–‡ "[xPatch: Dual-Stream Time Series Forecasting with Exponential Seasonal-Trend Decomposition](https://arxiv.org/pdf/2412.17323)" çš„æ¨¡å‹å¤ç°è¿‡ç¨‹å’Œä»£ç å®ç°è§£æã€‚xPatch æ˜¯ä¸€ä¸ªåŸºäºåŒæµæ¶æ„çš„æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹ï¼Œé€šè¿‡åˆ†è§£æ—¶é—´åºåˆ—ä¸ºå­£èŠ‚æ€§å’Œè¶‹åŠ¿æ€§ç»„ä»¶ï¼Œå¹¶åˆ†åˆ«é‡‡ç”¨éçº¿æ€§æµå’Œçº¿æ€§æµè¿›è¡Œå¤„ç†ã€‚æœ¬å¤ç°ç”± **Collin** å®Œæˆï¼ŒåŒ…å«å®Œæ•´çš„ä»£ç é‡æ„ã€æ¶æ„è§£æå’Œå®ç°ç»†èŠ‚è¯´æ˜ã€‚

**English Overview:**
This document provides a detailed record of the model reproduction process and code implementation analysis for the AAAI 2025 paper "[xPatch: Dual-Stream Time Series Forecasting with Exponential Seasonal-Trend Decomposition](https://arxiv.org/pdf/2412.17323)". xPatch is a dual-stream architecture-based time series forecasting model that decomposes time series into seasonal and trend components, processing them separately with non-linear and linear streams. This reproduction was completed by **Collin**, including complete code refactoring, architecture analysis, and implementation details.

## ğŸŒŸ å¤ç°ç‰¹æ€§ / Reproduction Features

### ä¸­æ–‡ç‰¹æ€§
- âœ… **å®Œæ•´é‡æ„**ï¼šåŸºäºè®ºæ–‡æ¶æ„å›¾ä»é›¶é‡æ–°å®ç°æ‰€æœ‰æ ¸å¿ƒæ¨¡å—
- âœ… **æ¨¡å—åŒ–è®¾è®¡**ï¼šæ¸…æ™°çš„æ¨¡å—åˆ’åˆ†ï¼Œæ¯ä¸ªç»„ä»¶èŒè´£æ˜ç¡®
- âœ… **è¯¦ç»†æ³¨é‡Š**ï¼šä»£ç ä¸­åŒ…å«ä¸°å¯Œçš„ä¸­è‹±æ–‡æ³¨é‡Šå’Œç»´åº¦æ ‡æ³¨
- âœ… **æ¶æ„è§£æ**ï¼šç»“åˆä»£ç æä¾›æ·±å…¥çš„æ¶æ„è®¾è®¡æ€è·¯åˆ†æ
- âœ… **ç»´åº¦è¿½è¸ª**ï¼šå®Œæ•´çš„æ•°æ®æµç»´åº¦å˜æ¢è¿‡ç¨‹è®°å½•
- âœ… **ç‹¬ç«‹è®­ç»ƒè„šæœ¬**ï¼šä¸“é—¨ä¸ºå¤ç°æ¨¡å‹å®šåˆ¶çš„è®­ç»ƒè„šæœ¬

### English Features
- âœ… **Complete Refactoring**: Reimplemented all core modules from scratch based on paper architecture diagrams
- âœ… **Modular Design**: Clear module separation with well-defined component responsibilities
- âœ… **Detailed Comments**: Rich bilingual comments and dimension annotations in code
- âœ… **Architecture Analysis**: In-depth analysis of architectural design concepts combined with code
- âœ… **Dimension Tracking**: Complete recording of data flow dimension transformation process
- âœ… **Independent Training Script**: Customized training script specifically for the reproduced model

## ğŸ—ï¸ æ¨¡å‹æ¶æ„æ¦‚è§ˆ / Model Architecture Overview

xPatch æ¨¡å‹çš„æ•´ä½“æ¶æ„å¯ä»¥åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªä¸»è¦éƒ¨åˆ†ï¼š

The overall xPatch model architecture can be divided into the following main components:

1. **é¢„å¤„ç†å±‚ / Preprocessing Layers**ï¼šRevIN æ ‡å‡†åŒ–å’Œæ—¶é—´åºåˆ—åˆ†è§£
2. **åŒæµå¤„ç† / Dual-Stream Processing**ï¼šéçº¿æ€§æµï¼ˆå¤„ç†å­£èŠ‚æ€§ï¼‰å’Œçº¿æ€§æµï¼ˆå¤„ç†è¶‹åŠ¿æ€§ï¼‰
3. **åå¤„ç†å±‚ / Post-processing Layers**ï¼šæµèåˆå’Œåæ ‡å‡†åŒ–

<p align="center">
<img src="./figures/dual-flow.png" alt="åŒæµæ¶æ„ç¤ºæ„å›¾" style="width: 80%;" align=center />
</p>

## ğŸ“‹ è¯¦ç»†ä»£ç è§£æ / Detailed Code Analysis

### 1. é¢„å¤„ç†å±‚ / Preprocessing Layers

#### 1.1 RevIN æ ‡å‡†åŒ– / RevIN Normalization

```python
# 1. Apply RevIN normalization if enabled
if self.revin:
    x = self.revin_layer(x,'norm')
```

**è®¾è®¡æ€è·¯ / Design Concept:**
- **ä¸­æ–‡**ï¼šRevINï¼ˆReversible Instance Normalizationï¼‰æ˜¯ä¸€ç§ä¸“é—¨ä¸ºæ—¶é—´åºåˆ—è®¾è®¡çš„æ ‡å‡†åŒ–æŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒæ—¶é—´åºåˆ—ç‰¹æ€§çš„åŒæ—¶è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ï¼Œé¿å…äº†ä¼ ç»Ÿæ ‡å‡†åŒ–å¯èƒ½ç ´åæ—¶é—´ä¾èµ–å…³ç³»çš„é—®é¢˜ã€‚
- **English**: RevIN (Reversible Instance Normalization) is a normalization technique specifically designed for time series, capable of normalization while preserving time series characteristics, avoiding the issue of traditional normalization potentially disrupting temporal dependencies.

#### 1.2 æ—¶é—´åºåˆ—åˆ†è§£ / Time Series Decomposition

```python
# 2. Decompose into seasonal and trend components. Input is 3D [B, L, C].
s,t = self.decomposition(x)
```

**å…³é”®åˆ›æ–° / Key Innovation:**
- **ä¸­æ–‡**ï¼šä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼ˆEMAï¼‰æˆ–åŒæŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼ˆDEMAï¼‰å°†è¾“å…¥æ—¶é—´åºåˆ—åˆ†è§£ä¸ºå­£èŠ‚æ€§ç»„ä»¶ï¼ˆsï¼‰å’Œè¶‹åŠ¿æ€§ç»„ä»¶ï¼ˆtï¼‰ï¼Œè¿™ç§åˆ†è§£æ–¹å¼ç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•èƒ½æ›´å¥½åœ°æ•è·æ—¶é—´åºåˆ—çš„å‘¨æœŸæ€§æ¨¡å¼ã€‚
- **English**: Uses Exponential Moving Average (EMA) or Double Exponential Moving Average (DEMA) to decompose input time series into seasonal component (s) and trend component (t). This decomposition method captures periodic patterns better than traditional approaches.

### 2. é€šé“ç‹¬ç«‹å¤„ç† / Channel Independence Processing

```python
# 3. Permute for channel independence: [B,L,C] -> [B,C,L]
s = s.permute(0,2,1)
t = t.permute(0,2,1)

# 4. Reshape for channel independence: [B,C,L] -> [B*C, L]
B, C, L = s.shape
s = s.reshape(B*C, L)
t = t.reshape(B*C, L)
```

**è®¾è®¡åŸç† / Design Principle:**
- **ä¸­æ–‡**ï¼šä¸ºäº†å®ç°é€šé“ç‹¬ç«‹å¤„ç†ï¼Œå°†ä¸‰ç»´å¼ é‡ `[æ‰¹æ¬¡, é•¿åº¦, é€šé“]` é‡æ„ä¸ºäºŒç»´å¼ é‡ `[æ‰¹æ¬¡*é€šé“, é•¿åº¦]`ï¼Œä½¿æ¯ä¸ªå˜é‡éƒ½èƒ½ç‹¬ç«‹å¤„ç†ï¼Œé¿å…äº†ä¸åŒå˜é‡é—´çš„ä¸å¿…è¦å¹²æ‰°ã€‚
- **English**: To achieve channel independence processing, reshapes 3D tensor `[Batch, Length, Channel]` into 2D tensor `[Batch*Channel, Length]`, allowing each variable to be processed independently and avoiding unnecessary interference between different variables.

### 3. éçº¿æ€§æµï¼ˆNon-Linear Streamï¼‰

éçº¿æ€§æµä¸“é—¨å¤„ç†å­£èŠ‚æ€§ç»„ä»¶ï¼Œé‡‡ç”¨åŸºäº Patch çš„æ¶æ„ï¼š

The non-linear stream specifically processes seasonal components using a Patch-based architecture:

#### 3.1 Patching å±‚ / Patching Layer

```python
class PathcingLayer(nn.Module):
    def __init__(self, seq_len:int, patch_len:int, stride:int, padding_patch:str):
        super().__init__()
        self.seq_len = seq_len # åºåˆ—é•¿åº¦
        self.patch_len = patch_len # æ¯ä¸ªpatchçš„é•¿åº¦
        self.stride = stride # æ»‘åŠ¨çª—å£çš„é•¿åº¦
        self.padding_patch = padding_patch # ç”¨äºåœ¨patchä¸­è¿›è¡Œpaddingçš„ç¬¦å·
        self.dim = patch_len * patch_len # patchembedding dim
        self.patch_num = (seq_len - patch_len) // stride + 1 # patchçš„æ•°é‡
        if padding_patch == 'end':
            self.padding_layer = nn.ReplicationPad1d((0,stride))
            self.patch_num += 1

    def forward(self,x):
        # step 3: Patching  [Batch*Channel, Input] -> [Batch*Channel, Patch_num,Patch_len]
        if self.padding_patch == 'end':
            x = self.padding_layer(x)
        x = x.unfold(dimension=-1, size=self.patch_len, step=self.stride)
        return x # [Batch*Channel, Patch_num,Patch_len]
```

**æ ¸å¿ƒè®¾è®¡æ€æƒ³ / Core Design Philosophy:**
- **ä¸­æ–‡**ï¼š
  - ä½¿ç”¨æ»‘åŠ¨çª—å£ `unfold` æ“ä½œå°†åºåˆ—åˆ†å‰²æˆé‡å çš„ patchesï¼Œç±»ä¼¼äº Vision Transformer çš„æ€æƒ³
  - `ReplicationPad1d` åœ¨åºåˆ—æœ«å°¾è¿›è¡Œå¡«å……ï¼Œç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½è¢«åˆ©ç”¨
  - è¾“å‡ºå½¢çŠ¶ï¼š`[B*C, patch_num, patch_len]`

- **English**:
  - Uses sliding window `unfold` operation to segment sequences into overlapping patches, similar to Vision Transformer concepts
  - `ReplicationPad1d` performs padding at sequence end to ensure all data is utilized
  - Output shape: `[B*C, patch_num, patch_len]`

#### 3.2 Patch åµŒå…¥å±‚ / Patch Embedding Layer

```python
class PatchingEmbedding(nn.Module):
    def __init__(self, patch_len:int, patch_num:int)->None:
        super().__init__()
        self.dim = patch_len * patch_len
        self.fc1 = nn.Linear(patch_len, self.dim)
        self.gelu = nn.GELU()
        self.bn1 = nn.BatchNorm1d(patch_num)

    def forward(self,x:torch.Tensor)->torch.Tensor:
        x = self.fc1(x) # [B*C, patch_num, patch_len] -> [B*C, patch_num, patch_len^2]
        x = self.gelu(x)
        x = self.bn1(x)
        return x
```

**è®¾è®¡äº®ç‚¹ / Design Highlights:**
- **ä¸­æ–‡**ï¼š
  - å°†æ¯ä¸ª patch ä» `patch_len` ç»´åº¦æ˜ å°„åˆ° `patch_lenÂ²` ç»´åº¦ï¼Œæå¤§å¢åŠ æ¨¡å‹å®¹é‡
  - ä½¿ç”¨ GELU æ¿€æ´»å‡½æ•°æä¾›æ›´å¥½çš„éçº¿æ€§è¡¨è¾¾èƒ½åŠ›
  - BatchNorm1d ç¡®ä¿è®­ç»ƒç¨³å®šæ€§

- **English**:
  - Maps each patch from `patch_len` dimension to `patch_lenÂ²` dimension, significantly increasing model capacity
  - Uses GELU activation function for better non-linear expression capability
  - BatchNorm1d ensures training stability

#### 3.3 æ®‹å·®è¿æ¥å— / Residual Connection Block

```python
class ResidualBlock(nn.Module):
    def __init__(self, patch_num:int, patch_len:int )->None:
        super().__init__()
        # Instantiate the two parallel layers/streams that will be combined
        self.residual_layer=ResidualLayer(patch_len)
        self.depthwise_conv=DepthwiseConv(patch_num, patch_len)

    def forward(self,x:torch.Tensor)->torch.Tensor:
        # 1.Calculate the output of the residual stream
        residual_out=self.residual_layer(x)
        # 2.Calculate the output of the main depthwise convolution stream
        depthwise_out=self.depthwise_conv(x)
        # 3. Add the outputs of the two streams together
        output = residual_out + depthwise_out
        return output
```

**æ¶æ„åˆ›æ–° / Architectural Innovation:**
- **ä¸­æ–‡**ï¼š
  - ç»“åˆæ®‹å·®è¿æ¥å’Œæ·±åº¦å¯åˆ†ç¦»å·ç§¯çš„è®¾è®¡æ€è·¯
  - `ResidualLayer` å°†é«˜ç»´åµŒå…¥æŠ•å½±å› `patch_len` ç»´åº¦ï¼Œæä¾›è·³è·ƒè¿æ¥
  - `DepthwiseConv` ä½¿ç”¨åˆ†ç»„å·ç§¯è¿›è¡Œç©ºé—´ç‰¹å¾æå–ï¼Œå‡å°‘å‚æ•°é‡åŒæ—¶ä¿æŒè¡¨è¾¾èƒ½åŠ›

- **English**:
  - Combines residual connection and depthwise separable convolution design concepts
  - `ResidualLayer` projects high-dimensional embedding back to `patch_len` dimension, providing skip connections
  - `DepthwiseConv` uses grouped convolution for spatial feature extraction, reducing parameters while maintaining expressiveness

#### 3.4 ç‚¹å·ç§¯å’Œ MLP å¤´ / Pointwise Convolution and MLP Head

```python
class PointwiseConv(nn.Module):
    def __init__(self,patch_num:int)->None:
        super().__init__()
        self.conv2 = nn.Conv1d(patch_num,patch_num,1,1)  # 1x1 å·ç§¯
        self.gelu = nn.GELU()
        self.bn3 = nn.BatchNorm1d(patch_num)

class MLPFlatten(nn.Module):
    def __init__(self,patch_num:int,patch_len:int,pred_len:int)->None:
        super().__init__()
        self.flatten = nn.Flatten(start_dim=-2)
        self.fc3 = nn.Linear(patch_num*patch_len, pred_len*2)
        self.gelu = nn.GELU()
        self.fc4 = nn.Linear(pred_len*2, pred_len)
```

**è®¾è®¡ç²¾é«“ / Design Essence:**
- **ä¸­æ–‡**ï¼š
  - ç‚¹å·ç§¯ï¼ˆ1x1å·ç§¯ï¼‰è¿›è¡Œé€šé“é—´ä¿¡æ¯äº¤äº’ï¼Œå¢å¼ºç‰¹å¾è¡¨è¾¾
  - MLP å¤´å°† patch ç‰¹å¾å±•å¹³å¹¶é€šè¿‡ä¸¤å±‚å…¨è¿æ¥ç½‘ç»œæ˜ å°„åˆ°é¢„æµ‹é•¿åº¦
  - ä¸­é—´å±‚ä½¿ç”¨ `pred_len*2` ç»´åº¦æä¾›å……åˆ†çš„ç‰¹å¾å˜æ¢ç©ºé—´

- **English**:
  - Pointwise convolution (1x1 convolution) for inter-channel information interaction, enhancing feature expression
  - MLP head flattens patch features and maps to prediction length through two-layer fully connected network
  - Intermediate layer uses `pred_len*2` dimension to provide sufficient feature transformation space

### 4. çº¿æ€§æµï¼ˆLinear Streamï¼‰

çº¿æ€§æµä¸“é—¨å¤„ç†è¶‹åŠ¿æ€§ç»„ä»¶ï¼Œé‡‡ç”¨ç®€å•è€Œæœ‰æ•ˆçš„ MLP æ¶æ„ï¼š

The linear stream specifically processes trend components using a simple yet effective MLP architecture:

```python
class LinearBlock(nn.Module):
    def __init__(self,seq_len:int,pred_len:int)->None:
        super().__init__()
        self.fc5 = nn.Linear(seq_len,pred_len*4)
        self.avgpool1 = nn.AvgPool1d(kernel_size=2)
        self.ln1 = nn.LayerNorm(pred_len*2)

        self.fc6 = nn.Linear(pred_len*2, pred_len)
        self.avgpool2 = nn.AvgPool1d(kernel_size=2)
        self.ln2 = nn.LayerNorm(pred_len//2)

        self.fc7 = nn.Linear(pred_len // 2, pred_len)

    def forward(self,x:torch.Tensor)->torch.Tensor:
        x = self.fc5(x)      # seq_len -> pred_len*4
        x = self.avgpool1(x) # pred_len*4 -> pred_len*2
        x = self.ln1(x)
        x = self.fc6(x)      # pred_len*2 -> pred_len
        x = self.avgpool2(x) # pred_len -> pred_len//2
        x = self.ln2(x)
        x = self.fc7(x)      # pred_len//2 -> pred_len
        return x
```

**è®¾è®¡å“²å­¦ / Design Philosophy:**
- **ä¸­æ–‡**ï¼š
  - å¤šå±‚ MLP ç»“æ„ï¼Œé€æ­¥é™ç»´å¤„ç†è¶‹åŠ¿ä¿¡æ¯
  - ä½¿ç”¨å¹³å‡æ± åŒ–è¿›è¡Œä¸‹é‡‡æ ·ï¼Œä¿æŒä¿¡æ¯çš„å¹³æ»‘æ€§
  - LayerNorm æä¾›æ­£åˆ™åŒ–ï¼Œé€‚åˆå¤„ç†å¹³æ»‘çš„è¶‹åŠ¿ä¿¡å·
  - ç®€å•æ¶æ„é€‚é…è¶‹åŠ¿æ•°æ®çš„ä½é¢‘ç‰¹æ€§

- **English**:
  - Multi-layer MLP structure with gradual dimension reduction for trend information processing
  - Uses average pooling for downsampling to maintain information smoothness
  - LayerNorm provides regularization, suitable for processing smooth trend signals
  - Simple architecture adapts to low-frequency characteristics of trend data

### 5. åŒæµèåˆ / Dual-Stream Fusion

```python
# 5. Process through respective streams
s_pre = self.nonlinear_block(s) # -> [B*C, pred_len]
t_pre = self.linear_block(t)

# 6. Concatenate stream outputs
x_out = torch.cat((s_pre,t_pre),dim=1) #->[B*C, pred_len*2]

# 7. Final Projection
x_out = self.fc8(x_out) # -> [B*C, pred_len]
```

**èåˆç­–ç•¥ / Fusion Strategy:**
- **ä¸­æ–‡**ï¼š
  - å°†ä¸¤ä¸ªæµçš„è¾“å‡ºåœ¨ç‰¹å¾ç»´åº¦ä¸Šè¿æ¥ï¼Œå®ç°ä¿¡æ¯äº’è¡¥
  - ä½¿ç”¨çº¿æ€§å±‚å°†èåˆç‰¹å¾æ˜ å°„åˆ°æœ€ç»ˆé¢„æµ‹ç»´åº¦
  - ç®€å•è€Œæœ‰æ•ˆçš„èåˆæ–¹å¼ï¼Œé¿å…è¿‡åº¦å¤æ‚åŒ–

- **English**:
  - Concatenates outputs from both streams in feature dimension for information complementarity
  - Uses linear layer to map fused features to final prediction dimension
  - Simple yet effective fusion approach, avoiding over-complication

### 6. åå¤„ç†ä¸å½¢çŠ¶æ¢å¤ / Post-processing and Shape Recovery

```python
# 8. Reshape back to include channels: [B*C, pred_len] -> [B,C,pred_len]
x_out = x_out.reshape(B,C,self.pred_len)

# 9. Permute back to original format: [B,C,pred_len] -> [B,pred_len,C]
x_out = x_out.permute(0,2,1)

# 10. Apply RevIn denormalization to get the final prediction
if self.revin:
    x_out = self.revin_layer(x_out,'denorm')
return x_out
```

**æ¢å¤æœºåˆ¶ / Recovery Mechanism:**
- **ä¸­æ–‡**ï¼š
  - æ¢å¤åŸå§‹çš„å¼ é‡å½¢çŠ¶å’Œç»´åº¦é¡ºåºï¼Œç¡®ä¿è¾“å‡ºæ ¼å¼ä¸€è‡´æ€§
  - ä½¿ç”¨ RevIN åæ ‡å‡†åŒ–æ¢å¤åŸå§‹æ•°æ®èŒƒå›´å’Œåˆ†å¸ƒ
  - å®Œæ•´çš„æ•°æ®æµé—­ç¯ï¼Œä¿è¯æ¨¡å‹çš„ç«¯åˆ°ç«¯å¯è®­ç»ƒæ€§

- **English**:
  - Restores original tensor shape and dimension order to ensure output format consistency
  - Uses RevIN denormalization to restore original data range and distribution
  - Complete data flow closed loop ensuring end-to-end trainability of the model

## ğŸ¯ æ¨¡å‹åˆå§‹åŒ–ä¸é…ç½® / Model Initialization and Configuration

```python
class XPatch(nn.Module):
    def __init__(self,seq_len:int, pred_len:int, enc_in:int, patch_len:int, stride:int,
                 alpha:float, ma_type:str="ema", beta:float=0.3, revin:bool=True)->None:
        super().__init__()
        self.pred_len=pred_len

        # 1. Instantiate pre-processing layers
        self.revin=revin
        self.revin_layer= RevIN(num_features=enc_in,affine=True,subtract_last=False)
        self.decomposition = DECOMP(ma_type=ma_type,alpha=alpha,beta=beta)

        # 2. Instantiate the two main network blocks
        self.nonlinear_block = NonLinearBlock(seq_len,patch_len,stride,pred_len)
        self.linear_block = LinearBlock(seq_len,pred_len)

        # 3. Streams Concatenation
        self.fc8 = nn.Linear(pred_len * 2, pred_len)
```

**å‚æ•°é…ç½®è¯´æ˜ / Parameter Configuration Description:**

| å‚æ•° / Parameter | è¯´æ˜ / Description |
|:---:|:---|
| `seq_len` | è¾“å…¥åºåˆ—é•¿åº¦ / Input sequence length |
| `pred_len` | é¢„æµ‹åºåˆ—é•¿åº¦ / Prediction sequence length |
| `enc_in` | è¾“å…¥ç‰¹å¾ç»´åº¦ï¼ˆå˜é‡æ•°é‡ï¼‰/ Input feature dimension (number of variables) |
| `patch_len` | Patch é•¿åº¦ / Patch length |
| `stride` | æ»‘åŠ¨çª—å£æ­¥é•¿ / Sliding window stride |
| `alpha` | EMA å¹³æ»‘ç³»æ•° / EMA smoothing coefficient |
| `ma_type` | ç§»åŠ¨å¹³å‡ç±»å‹ï¼ˆ'ema', 'dema', 'reg'ï¼‰/ Moving average type |
| `beta` | DEMA äºŒæ¬¡å¹³æ»‘ç³»æ•° / DEMA secondary smoothing coefficient |
| `revin` | æ˜¯å¦ä½¿ç”¨ RevIN æ ‡å‡†åŒ– / Whether to use RevIN normalization |

## ğŸ”¬ å…³é”®åˆ›æ–°ç‚¹åˆ†æ / Key Innovation Analysis

### 1. åŒæµæ¶æ„è®¾è®¡ / Dual-Stream Architecture Design

**ä¸­æ–‡åˆ†æï¼š**
- **åŠ¨æœº**ï¼šä¼ ç»Ÿæ—¶é—´åºåˆ—æ¨¡å‹å¾€å¾€ç”¨åŒä¸€å¥—å‚æ•°å¤„ç†æ‰€æœ‰é¢‘ç‡çš„ä¿¡æ¯ï¼Œè¿™å¯èƒ½å¯¼è‡´é«˜é¢‘å’Œä½é¢‘ç‰¹å¾ç›¸äº’å¹²æ‰°
- **è§£å†³æ–¹æ¡ˆ**ï¼šè®¾è®¡ä¸“é—¨çš„éçº¿æ€§æµå¤„ç†é«˜é¢‘å­£èŠ‚æ€§ä¿¡å·ï¼Œçº¿æ€§æµå¤„ç†ä½é¢‘è¶‹åŠ¿ä¿¡å·
- **ä¼˜åŠ¿**ï¼šå„è‡ªä¸“æ³¨äºæœ€é€‚åˆçš„é¢‘ç‡èŒƒå›´ï¼Œæé«˜æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›å’Œé¢„æµ‹ç²¾åº¦

**English Analysis:**
- **Motivation**: Traditional time series models often use the same set of parameters to process information of all frequencies, which may lead to interference between high-frequency and low-frequency features
- **Solution**: Design dedicated non-linear stream for high-frequency seasonal signals and linear stream for low-frequency trend signals
- **Advantage**: Each focuses on the most suitable frequency range, improving model expressiveness and prediction accuracy

### 2. Patch-based å¤„ç†æœºåˆ¶ / Patch-based Processing Mechanism

**ä¸­æ–‡åˆ†æï¼š**
- **çµæ„Ÿæ¥æº**ï¼šå€Ÿé‰´ Vision Transformer ä¸­å°†å›¾åƒåˆ†å‰²æˆ patches çš„æ€æƒ³
- **é€‚é…æ”¹è¿›**ï¼šé’ˆå¯¹ä¸€ç»´æ—¶é—´åºåˆ—æ•°æ®è¿›è¡Œæ»‘åŠ¨çª—å£åˆ†å‰²ï¼Œä¿æŒæ—¶é—´è¿ç»­æ€§
- **è®¡ç®—ä¼˜åŠ¿**ï¼šé™ä½åºåˆ—é•¿åº¦ç»´åº¦çš„è®¡ç®—å¤æ‚åº¦ï¼Œæé«˜è®­ç»ƒæ•ˆç‡

**English Analysis:**
- **Inspiration**: Borrows from Vision Transformer's idea of segmenting images into patches
- **Adaptation**: Applies sliding window segmentation for 1D time series data while maintaining temporal continuity
- **Computational Advantage**: Reduces computational complexity in sequence length dimension, improving training efficiency

### 3. é€šé“ç‹¬ç«‹å¤„ç† / Channel Independence Processing

**ä¸­æ–‡åˆ†æï¼š**
- **é—®é¢˜è¯†åˆ«**ï¼šä¸åŒå˜é‡å…·æœ‰ä¸åŒçš„ç»Ÿè®¡ç‰¹æ€§å’Œå˜åŒ–æ¨¡å¼
- **è®¾è®¡æ€è·¯**ï¼šé€šè¿‡ reshape æ“ä½œå®ç°æ¯ä¸ªå˜é‡çš„ç‹¬ç«‹å¤„ç†
- **å®é™…æ•ˆæœ**ï¼šé¿å…å˜é‡é—´çš„ç›¸äº’å¹²æ‰°ï¼Œæé«˜å¤šå˜é‡é¢„æµ‹çš„å‡†ç¡®æ€§

**English Analysis:**
- **Problem Identification**: Different variables have different statistical characteristics and variation patterns
- **Design Concept**: Achieves independent processing for each variable through reshape operations
- **Practical Effect**: Avoids interference between variables, improving accuracy of multivariate prediction

## ğŸ“Š æ•°æ®æµè¿½è¸ª / Data Flow Tracking

### å®Œæ•´çš„ç»´åº¦å˜æ¢è¿‡ç¨‹ / Complete Dimension Transformation Process

| æ­¥éª¤ / Step | æ“ä½œ / Operation | è¾“å…¥ç»´åº¦ / Input Dim | è¾“å‡ºç»´åº¦ / Output Dim | è¯´æ˜ / Description |
|:---:|:---|:---:|:---:|:---|
| 1 | RevIN æ ‡å‡†åŒ– / RevIN Norm | `[B, L, C]` | `[B, L, C]` | ä¿æŒç»´åº¦ä¸å˜ / Dimension unchanged |
| 2 | æ—¶é—´åºåˆ—åˆ†è§£ / Decomposition | `[B, L, C]` | `[B, L, C]`, `[B, L, C]` | åˆ†è§£ä¸ºå­£èŠ‚æ€§å’Œè¶‹åŠ¿æ€§ / Decompose to seasonal and trend |
| 3 | Permute | `[B, L, C]` | `[B, C, L]` | è°ƒæ•´ç»´åº¦é¡ºåº / Adjust dimension order |
| 4 | Reshape | `[B, C, L]` | `[B*C, L]` | å®ç°é€šé“ç‹¬ç«‹ / Achieve channel independence |
| 5 | Patching | `[B*C, L]` | `[B*C, N, P]` | åˆ†å‰²æˆ patches / Segment into patches |
| 6 | Patch Embedding | `[B*C, N, P]` | `[B*C, N, PÂ²]` | åµŒå…¥åˆ°é«˜ç»´ç©ºé—´ / Embed to high-dim space |
| 7 | éçº¿æ€§å¤„ç† / Non-linear | `[B*C, N, PÂ²]` | `[B*C, pred_len]` | å¤æ‚ç‰¹å¾æå– / Complex feature extraction |
| 8 | çº¿æ€§å¤„ç† / Linear | `[B*C, L]` | `[B*C, pred_len]` | ç®€å•è¶‹åŠ¿é¢„æµ‹ / Simple trend prediction |
| 9 | èåˆ / Fusion | `[B*C, pred_len*2]` | `[B*C, pred_len]` | åŒæµä¿¡æ¯èåˆ / Dual-stream fusion |
| 10 | æ¢å¤å½¢çŠ¶ / Reshape back | `[B*C, pred_len]` | `[B, pred_len, C]` | æ¢å¤åŸå§‹æ ¼å¼ / Restore original format |

## ğŸš€ ä½¿ç”¨è¯´æ˜ / Usage Instructions

### æ¨¡å‹å®ä¾‹åŒ– / Model Instantiation

```python
from models.model import XPatch

# åˆ›å»ºæ¨¡å‹å®ä¾‹ / Create model instance
model = XPatch(
    seq_len=96,          # è¾“å…¥åºåˆ—é•¿åº¦ / Input sequence length
    pred_len=96,         # é¢„æµ‹é•¿åº¦ / Prediction length  
    enc_in=8,            # è¾“å…¥ç‰¹å¾æ•° / Number of input features
    patch_len=16,        # Patch é•¿åº¦ / Patch length
    stride=8,            # æ­¥é•¿ / Stride
    alpha=0.3,           # EMA ç³»æ•° / EMA coefficient
    ma_type='ema',       # ç§»åŠ¨å¹³å‡ç±»å‹ / Moving average type
    beta=0.3,            # DEMA ç³»æ•° / DEMA coefficient
    revin=True           # ä½¿ç”¨ RevIN / Use RevIN
)

# å‰å‘ä¼ æ’­ç¤ºä¾‹ / Forward pass example
batch_size, seq_len, num_features = 32, 96, 8
x = torch.randn(batch_size, seq_len, num_features)
output = model(x)  # è¾“å‡ºå½¢çŠ¶: [32, 96, 8] / Output shape: [32, 96, 8]
```

### è®­ç»ƒè„šæœ¬ä½¿ç”¨ / Training Script Usage

```bash
# ä½¿ç”¨å®šåˆ¶è®­ç»ƒè„šæœ¬ / Use custom training script
python train_exchange_model.py

# å¸¦è‡ªå®šä¹‰å‚æ•° / With custom parameters
python train_exchange_model.py \
    --seq_len 96 \
    --pred_len 192 \
    --patch_len 16 \
    --stride 8 \
    --alpha 0.3 \
    --learning_rate 0.0001 \
    --train_epochs 100
```

## ğŸ¯ æ¨¡å‹ä¼˜åŠ¿ä¸ç‰¹ç‚¹ / Model Advantages and Features

### æŠ€æœ¯ä¼˜åŠ¿ / Technical Advantages

**ä¸­æ–‡ä¼˜åŠ¿ï¼š**
1. **è®¡ç®—æ•ˆç‡**ï¼šPatch-based å¤„ç†é™ä½äº†åºåˆ—é•¿åº¦çš„è®¡ç®—å¤æ‚åº¦
2. **è¡¨è¾¾èƒ½åŠ›**ï¼šåŒæµæ¶æ„èƒ½å¤ŸåŒæ—¶æ•è·å‘¨æœŸæ€§å’Œè¶‹åŠ¿æ€§æ¨¡å¼
3. **æ¨¡å‹è§£é‡Šæ€§**ï¼šæ˜ç¡®çš„åˆ†è§£å’Œå¤„ç†æµç¨‹ä½¿æ¨¡å‹è¡Œä¸ºæ›´å¯è§£é‡Š
4. **é€šç”¨æ€§**ï¼šé€šé“ç‹¬ç«‹å¤„ç†ä½¿æ¨¡å‹èƒ½å¤Ÿå¤„ç†ä¸åŒç‰¹æ€§çš„å¤šå˜é‡æ—¶é—´åºåˆ—
5. **è®­ç»ƒç¨³å®šæ€§**ï¼šå¤šç§æ­£åˆ™åŒ–æŠ€æœ¯ç¡®ä¿è®­ç»ƒè¿‡ç¨‹çš„ç¨³å®šæ€§

**English Advantages:**
1. **Computational Efficiency**: Patch-based processing reduces computational complexity of sequence length
2. **Expressiveness**: Dual-stream architecture can simultaneously capture periodic and trend patterns
3. **Model Interpretability**: Clear decomposition and processing flow makes model behavior more interpretable
4. **Generality**: Channel independence processing enables model to handle multivariate time series with different characteristics
5. **Training Stability**: Multiple regularization techniques ensure stability of training process

### é€‚ç”¨åœºæ™¯ / Applicable Scenarios

- **é‡‘èæ—¶é—´åºåˆ—**ï¼šè‚¡ä»·ã€æ±‡ç‡ç­‰å…·æœ‰æ˜æ˜¾è¶‹åŠ¿å’Œå‘¨æœŸæ€§çš„æ•°æ®
- **èƒ½æºé¢„æµ‹**ï¼šç”µåŠ›è´Ÿè·ã€å¯å†ç”Ÿèƒ½æºå‘ç”µé‡é¢„æµ‹
- **ä¾›åº”é“¾ç®¡ç†**ï¼šéœ€æ±‚é¢„æµ‹ã€åº“å­˜ä¼˜åŒ–
- **IoT ä¼ æ„Ÿå™¨æ•°æ®**ï¼šæ¸©åº¦ã€æ¹¿åº¦ç­‰ç¯å¢ƒç›‘æµ‹æ•°æ®

- **Financial Time Series**: Stock prices, exchange rates with obvious trends and periodicity
- **Energy Forecasting**: Power load, renewable energy generation forecasting
- **Supply Chain Management**: Demand forecasting, inventory optimization
- **IoT Sensor Data**: Temperature, humidity and other environmental monitoring data

## ğŸ”§ å®ç°ç»†èŠ‚ä¸æ³¨æ„äº‹é¡¹ / Implementation Details and Considerations

### é‡è¦å®ç°ç»†èŠ‚ / Important Implementation Details

**ä¸­æ–‡è¯´æ˜ï¼š**
1. **ç»´åº¦å˜æ¢**ï¼šç¡®ä¿æ‰€æœ‰ permute å’Œ reshape æ“ä½œçš„ç»´åº¦å¯¹åº”æ­£ç¡®
2. **è®¾å¤‡å…¼å®¹æ€§**ï¼šåœ¨ MPSï¼ˆApple Siliconï¼‰è®¾å¤‡ä¸Šéœ€è¦æ³¨æ„ float64 å…¼å®¹æ€§é—®é¢˜
3. **æ¢¯åº¦è®¡ç®—**ï¼š`unfold` æ“ä½œåœ¨æŸäº›è®¾å¤‡ä¸Šçš„åå‘ä¼ æ’­æ”¯æŒæœ‰é™
4. **å†…å­˜ç®¡ç†**ï¼šé€šé“ç‹¬ç«‹å¤„ç†ä¼šå¢åŠ å†…å­˜ä½¿ç”¨ï¼Œéœ€è¦åˆç†è®¾ç½® batch_size

**English Notes:**
1. **Dimension Transformation**: Ensure correct dimension correspondence for all permute and reshape operations
2. **Device Compatibility**: Need to pay attention to float64 compatibility issues on MPS (Apple Silicon) devices
3. **Gradient Computation**: Limited backward propagation support for `unfold` operations on certain devices
4. **Memory Management**: Channel independence processing increases memory usage, need to set batch_size reasonably

### è°ƒè¯•å»ºè®® / Debugging Suggestions

```python
# æ·»åŠ ç»´åº¦æ£€æŸ¥ç‚¹ / Add dimension checkpoints
def debug_forward(self, x):
    print(f"Input shape: {x.shape}")
    
    # RevIN
    if self.revin:
        x = self.revin_layer(x, 'norm')
        print(f"After RevIN: {x.shape}")
    
    # Decomposition
    s, t = self.decomposition(x)
    print(f"After decomposition - s: {s.shape}, t: {t.shape}")
    
    # Channel independence
    s = s.permute(0,2,1).reshape(-1, s.shape[1])
    t = t.permute(0,2,1).reshape(-1, t.shape[1])
    print(f"After reshape - s: {s.shape}, t: {t.shape}")
    
    # Stream processing
    s_out = self.nonlinear_block(s)
    t_out = self.linear_block(t)
    print(f"Stream outputs - s: {s_out.shape}, t: {t_out.shape}")
    
    return s_out, t_out
```

## ğŸ“š å‚è€ƒèµ„æ–™ä¸è‡´è°¢ / References and Acknowledgements

### åŸå§‹è®ºæ–‡ / Original Paper
```bibtex
@inproceedings{stitsyuk2025xpatch,
  title={xPatch: Dual-Stream Time Series Forecasting with Exponential Seasonal-Trend Decomposition},
  author={Stitsyuk, Artyom and Choi, Jaesik},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={39},
  number={19},
  pages={20601--20609},
  year={2025}
}
```

### ç›¸å…³å¼€æºé¡¹ç›® / Related Open Source Projects
- [Time-Series-Library](https://github.com/thuml/Time-Series-Library)
- [PatchTST](https://github.com/yuqinie98/PatchTST)
- [Autoformer](https://github.com/thuml/Autoformer)

## ğŸ“§ è”ç³»æ–¹å¼ / Contact Information

**å¤ç°æ•´ç†ï¼šCollin / Reproduced and Organized by: Collin**

å¦‚æœ‰ä»»ä½•å…³äºå¤ç°å®ç°çš„é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»ï¼š

For any questions or suggestions regarding the reproduction implementation, please contact via:

- **GitHub Issues**: [æäº¤é—®é¢˜ / Submit Issue](https://github.com/awesome-ai-papers-reproduction/issues)
- **Email**: junnebailiu@gmail.com

---

<div align="center">
  <p><strong>ğŸŒŸ æœ¬å¤ç°å®ç°å®Œå…¨å¼€æºï¼Œæ¬¢è¿å­¦ä¹ äº¤æµï¼/ This reproduction implementation is fully open source, welcome to learn and exchange! ğŸŒŸ</strong></p>
  <p><strong>ğŸ“ å¤ç°æ•´ç†ï¼šCollin | åŸºäº AAAI 2025 xPatch è®ºæ–‡ / Reproduced by: Collin | Based on AAAI 2025 xPatch Paper</strong></p>
</div> 